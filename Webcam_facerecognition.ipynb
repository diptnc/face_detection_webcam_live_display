{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e832b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### General imports ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from time import sleep\n",
    "import re\n",
    "import os\n",
    "import argparse\n",
    "from collections import OrderedDict\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "### Image processing ###\n",
    "from scipy.ndimage import zoom\n",
    "from scipy.spatial import distance\n",
    "import imutils\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "import dlib\n",
    "# from __future__ import division\n",
    "from imutils import face_utils\n",
    "\n",
    "### CNN models ###\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, SeparableConv2D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2#, activity_l2\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import models\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "### Build SVM models ###\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "\n",
    "### Same trained models ###\n",
    "import h5py\n",
    "from keras.models import model_from_json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d51f17d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_x = 48\n",
    "shape_y = 48\n",
    "\n",
    "def detect_face(frame):\n",
    "    \n",
    "    #Cascade classifier pre-trained model\n",
    "    cascPath = './haarcascade_frontalface_default.xml'\n",
    "    faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "    \n",
    "    #BGR -> Gray conversion\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Cascade MultiScale classifier\n",
    "    detected_faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=6,\n",
    "                                                  minSize=(shape_x, shape_y),\n",
    "                                                  flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    coord = []\n",
    "    \n",
    "    for x, y, w, h in detected_faces :\n",
    "        if w > 100 :\n",
    "            sub_img=frame[y:y+h,x:x+w]\n",
    "            #cv2.rectangle(frame,(x,y),(x+w,y+h),(0, 255,255),1)\n",
    "            coord.append([x,y,w,h])\n",
    "    \n",
    "    return gray, detected_faces, coord\n",
    "\n",
    "#Extraire les features faciales\n",
    "def extract_face_features(faces, offset_coefficients=(0.075, 0.05)):\n",
    "    gray = faces[0]\n",
    "    detected_face = faces[1]\n",
    "    \n",
    "    new_face = []\n",
    "    \n",
    "    for det in detected_face :\n",
    "        #Region dans laquelle la face est détectée\n",
    "        x, y, w, h = det\n",
    "        #X et y correspondent à la conversion en gris par gray, et w, h correspondent à la hauteur/largeur\n",
    "    \n",
    "        #Offset coefficient, np.floor takes the lowest integer (delete border of the image)\n",
    "        horizontal_offset = np.int(np.floor(offset_coefficients[0] * w))\n",
    "        vertical_offset = np.int(np.floor(offset_coefficients[1] * h))\n",
    "\n",
    "        #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        #gray transforme l'image\n",
    "        extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
    "    \n",
    "        #Zoom sur la face extraite\n",
    "        new_extracted_face = zoom(extracted_face, (shape_x / extracted_face.shape[0],shape_y / extracted_face.shape[1]))\n",
    "        #cast type float\n",
    "        new_extracted_face = new_extracted_face.astype(np.float32)\n",
    "        #scale\n",
    "        new_extracted_face /= float(new_extracted_face.max())\n",
    "        #print(new_extracted_face)\n",
    "    \n",
    "        new_face.append(new_extracted_face)\n",
    "    \n",
    "    return new_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cddad95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model('./model[ekaggle].h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b1d61f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf2742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31065bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad20f5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed4892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b079b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "\tA = distance.euclidean(eye[1], eye[5])\n",
    "\tB = distance.euclidean(eye[2], eye[4])\n",
    "\tC = distance.euclidean(eye[0], eye[3])\n",
    "\tear = (A + B) / (2.0 * C)\n",
    "\treturn ear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7d6b938",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.25\n",
    "frame_check = 20\n",
    "face_detect = dlib.get_frontal_face_detector()\n",
    "predictor_landmarks = dlib.shape_predictor(\"./face_landmarks.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92743f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(lStart, lEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "(rStart, rEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eye\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "510f01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(nStart, nEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"nose\"]\n",
    "(mStart, mEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"mouth\"]\n",
    "(jStart, jEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"jaw\"]\n",
    "\n",
    "(eblStart, eblEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"left_eyebrow\"]\n",
    "(ebrStart, ebrEnd) = face_utils.FACIAL_LANDMARKS_IDXS[\"right_eyebrow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab670bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "desiredLeftEye=(0.35, 0.35)\n",
    "\n",
    "def align(gray, rect):\n",
    "    # convert the landmark (x, y)-coordinates to a NumPy array\n",
    "    shape = predictor(gray, rect)\n",
    "    shape = shape_to_np(shape)\n",
    " \n",
    "    # extract the left and right eye (x, y)-coordinates\n",
    "    (lStart, lEnd) = FACIAL_LANDMARKS_IDXS[\"left_eye\"]\n",
    "    (rStart, rEnd) = FACIAL_LANDMARKS_IDXS[\"right_eye\"]\n",
    "    leftEyePts = shape[lStart:lEnd]\n",
    "    rightEyePts = shape[rStart:rEnd]\n",
    "        \n",
    "    # compute the center of mass for each eye\n",
    "    leftEyeCenter = leftEyePts.mean(axis=0).astype(\"int\")\n",
    "    rightEyeCenter = rightEyePts.mean(axis=0).astype(\"int\")\n",
    " \n",
    "    # compute the angle between the eye centroids\n",
    "    dY = rightEyeCenter[1] - leftEyeCenter[1]\n",
    "    dX = rightEyeCenter[0] - leftEyeCenter[0]\n",
    "    angle = np.degrees(np.arctan2(dY, dX)) - 180\n",
    "\n",
    "    desiredRightEyeX = 1.0 - desiredLeftEye[0]\n",
    " \n",
    "\n",
    "    dist = np.sqrt((dX ** 2) + (dY ** 2))\n",
    "    desiredDist = (desiredRightEyeX - desiredLeftEye[0])\n",
    "    desiredDist *= self.desiredFaceWidth\n",
    "    scale = desiredDist / dist\n",
    "        \n",
    "    # compute center (x, y)-coordinates (i.e., the median point)\n",
    "    # between the two eyes in the input image\n",
    "    eyesCenter = ((leftEyeCenter[0] + rightEyeCenter[0]) // 2,\n",
    "            (leftEyeCenter[1] + rightEyeCenter[1]) // 2)\n",
    " \n",
    "    # grab the rotation matrix for rotating and scaling the face\n",
    "    M = cv2.getRotationMatrix2D(eyesCenter, angle, scale)\n",
    " \n",
    "    # update the translation component of the matrix\n",
    "    tX = self.desiredFaceWidth * 0.5\n",
    "    tY = self.desiredFaceHeight * self.desiredLeftEye[1]\n",
    "    M[0, 2] += (tX - eyesCenter[0])\n",
    "    M[1, 2] += (tY - eyesCenter[1])\n",
    "        \n",
    "    # apply the affine transformation\n",
    "    (w, h) = (self.desiredFaceWidth, self.desiredFaceHeight)\n",
    "    #output = cv2.warpAffine(image, M, (w, h), flags=cv2.INTER_CUBIC)\n",
    "    output = cv2.warpAffine(gray, M, (w, h), flags=cv2.INTER_CUBIC)\n",
    "    # return the aligned face\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85568fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c3a539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lancer la capture video\n",
    "\n",
    "COLOR_PRED=(0,0,255)\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "flag = 0\n",
    "j = 1\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    \n",
    "    face_index = 0\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    rects = face_detect(gray, 1)\n",
    "    #gray, detected_faces, coord = detect_face(frame)\n",
    "    \n",
    "    for (i, rect) in enumerate(rects):\n",
    "        \n",
    "        shape = predictor_landmarks(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "    \n",
    "        (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "        face = gray[y:y+h,x:x+w]\n",
    "        \n",
    "        #Zoom sur la face extraite\n",
    "        face = zoom(face, (shape_x / face.shape[0],shape_y / face.shape[1]))\n",
    "        #cast type float\n",
    "        face = face.astype(np.float32)\n",
    "        #scale\n",
    "        face /= float(face.max())\n",
    "        face = np.reshape(face.flatten(), (1, 48, 48, 1))\n",
    "        prediction = model.predict(face)\n",
    "        prediction_result = np.argmax(prediction)\n",
    "        \n",
    "        # Rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "    \n",
    "        cv2.putText(frame, \"Face #{}\".format(i + 1), (x - 10, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.rectangle(frame,(20,100),(220,280),(255,255,255,0.5),-1)\n",
    "\n",
    "        \n",
    "        \n",
    "         # 12. Add prediction probabilities\n",
    "        cv2.putText(frame, \"Emotional report : \",(40,120), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_PRED, 0)\n",
    "        cv2.putText(frame, \"Angry : \" + str(round(prediction[0][0],3)),(40,140), cv2.FONT_HERSHEY_SIMPLEX, 0.5,COLOR_PRED, 0)\n",
    "        cv2.putText(frame, \"Disgust : \" + str(round(prediction[0][1],3)),(40,160), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_PRED, 0)\n",
    "        cv2.putText(frame, \"Fear : \" + str(round(prediction[0][2],3)),(40,180), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_PRED, 1)\n",
    "        cv2.putText(frame, \"Happy : \" + str(round(prediction[0][3],3)),(40,200), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_PRED, 1)\n",
    "        cv2.putText(frame, \"Sad : \" + str(round(prediction[0][4],3)),(40,220), cv2.FONT_HERSHEY_SIMPLEX, 0.5,COLOR_PRED, 1)\n",
    "        cv2.putText(frame, \"Surprise : \" + str(round(prediction[0][5],3)),(40,240), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_PRED, 1)\n",
    "        cv2.putText(frame, \"Neutral : \" + str(round(prediction[0][6],3)),(40,260), cv2.FONT_HERSHEY_SIMPLEX, 0.5, COLOR_PRED, 1)\n",
    "        \n",
    "\n",
    "                   \n",
    "        if prediction_result == 0 :\n",
    "            cv2.putText(frame, \"Angry : \" + str(round((prediction[0][0])*100,3))+\" %\",(x,y+h+30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 255, 0), 2)\n",
    "#             cv2.putText(frame, \"Angry\",(x+w-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        elif prediction_result == 1 :\n",
    "            cv2.putText(frame, \"Disgust : \" + str(round((prediction[0][0])*100,3))+\" %\",(x,y+h+30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 255, 0), 2)\n",
    "        elif prediction_result == 2 :\n",
    "            cv2.putText(frame, \"Fear : \" + str(round((prediction[0][0])*100,3))+\" %\",(x,y+h+30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 255, 0), 2)\n",
    "        elif prediction_result == 3 :\n",
    "            cv2.putText(frame, \"Happy : \" + str(round((prediction[0][3])*100,3))+\" %\",(x,y+h+30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 255, 0), 2)\n",
    "        elif prediction_result == 4 :\n",
    "            cv2.putText(frame, \"Sad : \" + str(round((prediction[0][0])*100,3))+\" %\",(x,y+h+30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 255, 0), 2)\n",
    "        elif prediction_result == 5 :\n",
    "            cv2.putText(frame, \"Surprise : \" + str(round((prediction[0][0])*100,3))+\" %\",(x,y+h+30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 255, 0), 2)\n",
    "        else :\n",
    "            cv2.putText(frame, \"Neutral : \" + str(round((prediction[0][0])*100,3))+\" %\",(x,y+h+30), cv2.FONT_HERSHEY_SIMPLEX, 0.5,(0, 255, 0), 2)\n",
    "                   \n",
    "       \n",
    "        cv2.putText(frame,'Number of Faces : ' + str(i+1),(40, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, COLOR_PRED, 4)\n",
    "        j = j + 1\n",
    "        cv2.imshow('Video', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac7b170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
